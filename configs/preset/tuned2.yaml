# @package _global_

model:
  alpha_drop:               0.0         # [0.0, 0.1]
  drop_path_rate:           0.0         # [0.0, 0.05]
  proj_drop:                0.0

training:
  hessian_loss_type: "mae"

  # lr_schedule_type: "cosine"
  lr_schedule_config:
    T_max: 300 # Maximum number of epochs.
    eta_min: 7e-5 # Minimum learning rate. Default: 0.
    warmup_epochs: 10          # number of epochs to warm up (0 disables)

optimizer:
  amsgrad: False
  beta1: 0.965
  beta2: 0.965
  lr: 0.00044
  weight_decay: 2e-05

pltrainer:
  gradient_clip_algorithm: norm
  gradient_clip_val: 7