# @package _global_

model:
  alpha_drop:               0.0         # [0.0, 0.1]
  drop_path_rate:           0.0         # [0.0, 0.05]
  proj_drop:                0.0

training:
  hessian_loss_type: "mae"

optimizer:
  amsgrad: False
  beta1: 0.965
  beta2: 0.965
  lr: 0.00044
  weight_decay: 2e-05

pltrainer:
  gradient_clip_algorithm: norm
  gradient_clip_val: 7