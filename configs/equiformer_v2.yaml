# @package _global_
# from https://github.com/deepprinciple/HORM

# ocp_trainer: forces_v2


model:
  name: EquiformerV2

  use_pbc:                  False
  regress_forces:           True
  otf_graph:                True
  max_neighbors:            20
  max_radius:               12.0
  max_num_elements:         90

  # avg_num_nodes:            77.81317
  avg_degree:               23.395238876342773  # IS2RE: 100k, max_radius = 5, max_neighbors = 100
  avg_degree_hessian:       23.395238876342773

  avg_num_nodes:            null
  # HORM T1x
  # Number of edges: 176.7360948021343
  # Number of edges (hessian): 186.7841774653667
  # Number of atoms: 13.919938540433833
  # _AVG_DEGREE_HESSIAN = 13.4184627987
  # _AVG_DEGREE = 12.6966145927


  num_layers:               4
  sphere_channels:          128
  attn_hidden_channels:     64              # [64, 96] This determines the hidden size of message passing. Do not necessarily use 96.
  num_heads:                4
  attn_alpha_channels:      64              # Not used when `use_s2_act_attn` is True.
  attn_value_channels:      16
  ffn_hidden_channels:      128
  norm_type:                'layer_norm_sh' # ['rms_norm_sh', 'layer_norm', 'layer_norm_sh']

  lmax_list:                [4]
  mmax_list:                [2]
  grid_resolution:          18              # [18, 16, 14, None] For `None`, simply comment this line.

  num_sphere_samples:       128

  edge_channels:              128
  use_atom_edge_embedding:    True
  # If `True`, `use_atom_edge_embedding` must be `True` and the atom edge embedding will be shared across all blocks. 
  share_atom_edge_embedding:  False         
  distance_function:          'gaussian'
  num_distance_basis:         512         # not used

  attn_activation:          'silu'
  use_s2_act_attn:          False       # [False, True] Switch between attention after S2 activation or the original EquiformerV1 attention. 
  use_attn_renorm:          True        # Attention re-normalization. Used for ablation study.
  ffn_activation:           'silu'      # ['silu', 'swiglu']
  use_gate_act:             False       # [True, False] Switch between gate activation and S2 activation
  use_grid_mlp:             True        # [False, True] If `True`, use projecting to grids and performing MLPs for FFNs.
  use_sep_s2_act:           True        # Separable S2 activation. Used for ablation study.
  
  alpha_drop:               0.1         # [0.0, 0.1]
  drop_path_rate:           0.1         # [0.0, 0.05]
  proj_drop:                0.0

  weight_init:              'uniform'    # ['uniform', 'normal']
