defaults:
  - ./equiformer_v2
  - _self_
  - lr: cosine
  # - trgt: hessian
  - preset: mae
  - experiment: t1x
  - gpu: one

# Model configuration
model_type: "EquiformerV2"
version: "0.1"
project: "hip"
potential_module_class: "PotentialModule"

model:
  hessian_alpha_drop: 0.0
  num_layers_hessian: 1

  cutoff: 5.0
  max_neighbors: 100000
  cutoff_hessian: 100.0

  hessian_no_attn_weights: True # build Hessian off-diagonal elements from raw messages without attention weights
  attn_wo_sigmoid: False # do not apply sigmoid to attention weights


# Optimizer configuration
optimizer:
  optimizer: "adamw" # adamw or muon, but muon has only ~20 2d params 
  lr: 3e-4  
  betas: [0.8, 0.95] # [0.9, 0.999], [0.9, 0.95], [0.8, 0.95]
  beta1: null
  beta2: null
  weight_decay: 1e-5 # 0.01, 1e-5
  amsgrad: false # true
  weight_decay_muon: 0.01 # 0.01, 1e-5
  lr_muon: 0.02 # 0.02, 3e-4

# Training configuration  
training:
  bz: 128
  bz_val: 128
  num_workers: 2 # 48
  clip_grad: true
  data_weight: null
  keep_fluorine: false 

  # horm used: loss = floss * 100 + eloss * 4 + hessian_loss * 4
  hessian_loss_weight: 10.0
  energy_loss_weight: 1.0
  force_loss_weight: 25.0
  hessian_loss_type: "mae"

  follow_batch: ["diag_ij", "edge_index", "message_idx_ij"]

  eigen_loss:
    k: null
    alpha: null

  # if true, freezes the backbone and only trains the Hessian head
  train_hessian_only: false

  otfgraph_in_model: true

  # Checkpoint loading options
  load_weights_only: false  # If true, only load model weights, ignore training state (epoch, optimizer, etc.)

  max_train_samples: null
  max_val_samples: null
  drop_last: true # otherwise we run into shape issues

# Pytorch Lightning Trainer configuration
pltrainer:
  max_epochs: 10000
  gradient_clip_val: 0.1
  gradient_clip_algorithm: "norm"
  accumulate_grad_batches: 1
  limit_train_batches: 0.1 # use 10%
  limit_val_batches: 0.1 
  log_every_n_steps: 1
  check_val_every_n_epoch: 4  # Run validation every N epochs (default: 1)
  # val_check_interval: 0.5    # Alternative: Run validation every N steps or fraction of epoch
  # overfit_batches: 2

early_stopping:
  monitor: "val-totloss"
  patience: 1000
  mode: "min"
  verbose: true

use_wandb: true

run_name: null # used for wandb and checkpointing
ckpt_base_dir: "checkpoint/hip"
ckpt_model_path: null # horm for finetuning
# Checkpoint that contains model weights and optimizerstate , lr state, etc. 
ckpt_trainer_path: null
ckpt_resume_auto: true
ckpt_do_save: true

# will be overwritten later
slurm_job_id: null

# variables we can access in our code
job_name: 'results'
# job_name: ${hydra:job.name}
config_name: ${hydra:job.config_name}
# Stores the command line arguments overrides
override_dirname: ${hydra:job.override_dirname}
# Changes the current working directory to the output directory for each job
# hydra.job.chdir: False