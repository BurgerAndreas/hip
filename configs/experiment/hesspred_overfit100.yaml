# @package _global_

# Overfit experiment configuration for tiny dataset
# This config is designed to overfit to sample_100.lmdb quickly

training:
  trn_path: "data/sample_100-hesspred.lmdb"
  val_path: "data/sample_100-hesspred.lmdb"
  bz: 100
  bz_val: 100
  # lr_schedule_type: null  # No learning rate decay for overfitting
  max_epochs: 100000

  # already preprocessed dataset, don't need to do hessian graph transform
  do_hessiangraphtransform: false

  
  lr_schedule_type: "step"
  lr_schedule_config:
    gamma: 0.85
    step_size: 400

model:
  alpha_drop:               0.0         # [0.0, 0.1]
  drop_path_rate:           0.0         # [0.0, 0.05]
  proj_drop:                0.0

pltrainer:
  limit_train_batches: null  # Remove limit to see full tiny dataset 
  limit_val_batches: null    # Remove validation limit
  # val_check_interval: 100    # Validate less frequently
  check_val_every_n_epoch: 100 # Validate every 10 epochs
  log_every_n_steps: 100

# no need to resume or save checkpoints
ckpt_resume_auto: false
ckpt_do_save: false
ckpt_trainer_path: null

experiment_name: "hesspred_overfit100"